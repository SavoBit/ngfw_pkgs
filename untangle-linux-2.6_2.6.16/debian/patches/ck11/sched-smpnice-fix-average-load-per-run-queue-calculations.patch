
From: Peter Williams <pwil3058@bigpond.net.au>

The logical value to use for the average load per task on a run queue
without any runnable tasks is the "default" load for a nice==0 task. 
Failure to do this may lead to anomalies in try_to_wake_up(), etc.

Signed-off-by: Peter Williams <pwil3058@bigpond.com.au>
Cc: "Siddha, Suresh B" <suresh.b.siddha@intel.com>
Cc: "Chen, Kenneth W" <kenneth.w.chen@intel.com>
Cc: Ingo Molnar <mingo@elte.hu>
Cc: Nick Piggin <nickpiggin@yahoo.com.au>
Cc: Con Kolivas <kernel@kolivas.org>
Cc: John Hawkes <hawkes@sgi.com>
Signed-off-by: Andrew Morton <akpm@osdl.org>
 kernel/sched.c |    4 +++-
 1 files changed, 3 insertions(+), 1 deletion(-)

Index: linux-2.6.16-ck11/kernel/sched.c
===================================================================
--- linux-2.6.16-ck11.orig/kernel/sched.c	2006-05-21 12:20:00.000000000 +1000
+++ linux-2.6.16-ck11/kernel/sched.c	2006-05-21 12:20:00.000000000 +1000
@@ -1058,7 +1058,7 @@ static inline unsigned long cpu_avg_load
 	runqueue_t *rq = cpu_rq(cpu);
 	unsigned long n = rq->nr_running;
 
-	return n ?  rq->raw_weighted_load / n : rq->raw_weighted_load;
+	return n ?  rq->raw_weighted_load / n : SCHED_LOAD_SCALE;
 }
 
 /*
@@ -2146,6 +2146,8 @@ find_busiest_group(struct sched_domain *
 			min(busiest_load_per_task, max_load);
 		if (this_nr_running)
 			this_load_per_task /= this_nr_running;
+		else
+			this_load_per_task = SCHED_LOAD_SCALE;
 		pwr_now += this->cpu_power *
 			min(this_load_per_task, this_load);
 		pwr_now /= SCHED_LOAD_SCALE;
